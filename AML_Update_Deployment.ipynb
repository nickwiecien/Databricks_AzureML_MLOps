{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d213d92-3791-47e6-b403-4e0a44f24cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Azure Databricks / Azure Machine Learning Sample - Deployment Update\n",
    "\n",
    "Sample notebook showcasing how to promote a staged model to a full-production slot (100% traffic allocation) and remove the previous deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abbfa75-c18d-4070-aed5-bff58ac0ad48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbd52b4-f3a9-495b-8200-fd3ae086c708",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql import SparkSession  \n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.deployments import get_deploy_client\n",
    "import json\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f433ba-6c98-4363-94fc-d41ed9371be9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000727f0-0660-4d9f-ba45-7d332150cb3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subscription_id = dbutils.widgets.get('subscription_id')\n",
    "resource_group = dbutils.widgets.get('resource_group')\n",
    "workspace = dbutils.widgets.get('workspace')\n",
    "\n",
    "model_name = dbutils.widgets.get('model_name')\n",
    "endpoint_name = dbutils.widgets.get('endpoint_name')\n",
    "endpoint_description = dbutils.widgets.get('endpoint_description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae20f98-62bc-473c-8125-b86bcb3544c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0e9a15-616c-485c-9560-f297fbd4e161",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_aml_client(subscription_id, resource_group, workspace):\n",
    "    \"\"\"\n",
    "    This function establishes a connection to an Azure Machine Learning (AML) workspace using a service principal.\n",
    "    It retrieves the tenant ID, client ID, and client secret from a Databricks secret scope and returns an AML client object.\n",
    "\n",
    "    Args:  \n",
    "        subscription_id (str): The Azure subscription ID.  \n",
    "        resource_group (str): The Azure resource group name.  \n",
    "        workspace (str): The Azure Machine Learning workspace name.  \n",
    "\n",
    "    Returns:  \n",
    "        ml_client (azure.ml.core.client.MLClient): An Azure ML client object with an established connection to the specified AML workspace.  \n",
    "    \"\"\"  \n",
    "    from azure.identity import ClientSecretCredential, DefaultAzureCredential\n",
    "    import os\n",
    "\n",
    "    tenant_id = dbutils.secrets.get(scope=\"amlsecretscope\",key=\"tenantid\")\n",
    "    client_id = dbutils.secrets.get(scope=\"amlsecretscope\",key=\"clientid\")\n",
    "    client_secret = dbutils.secrets.get(scope=\"amlsecretscope\",key=\"clientsecret\")\n",
    "    \n",
    "    os.environ[\"AZURE_TENANT_ID\"] = tenant_id\n",
    "    os.environ[\"AZURE_CLIENT_ID\"] = client_id\n",
    "    os.environ[\"AZURE_CLIENT_SECRET\"] = client_secret\n",
    "\n",
    "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
    "\n",
    "    ml_client = MLClient(\n",
    "        credential, subscription_id, resource_group, workspace\n",
    "    )\n",
    "    print(\"Establishing connection to Azure ML workspace\")\n",
    "    return ml_client\n",
    "    \n",
    "\n",
    "def get_aml_mlflow_tracking_uri(ml_client):\n",
    "    \"\"\"\n",
    "    This function retrieves the MLflow tracking URI for an Azure Machine Learning workspace.\n",
    "\n",
    "    Args:  \n",
    "        ml_client (object): The ml_client which references the target Azure Machine Learning workspace.  \n",
    "\n",
    "    Returns:  \n",
    "        tracking_uri (str): The MLflow tracking URI associated with the Azure Machine Learning workspace.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    ws = ml_client.workspaces.get(workspace)\n",
    "    return ws.mlflow_tracking_uri\n",
    "\n",
    "def split_train_test_data(pandas_df, training_percent=0.8):\n",
    "    \"\"\"\n",
    "    This function splits a Pandas DataFrame into training and testing datasets using the specified training percentage.\n",
    "\n",
    "    Args:  \n",
    "        pandas_df (pd.DataFrame): The input Pandas DataFrame to be split.  \n",
    "        training_percent (float, optional): The percentage of data to be used for training. Defaults to 0.8.  \n",
    "\n",
    "    Returns:  \n",
    "        train_df (pd.DataFrame): The training dataset as a Pandas DataFrame.  \n",
    "        test_df (pd.DataFrame): The testing dataset as a Pandas DataFrame.  \n",
    "    \"\"\"  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(pandas_df, test_size = 1.0 - training_percent)\n",
    "    print('Splitting data into train/test subsets')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def get_deployments(endpoint_name, ml_client):\n",
    "    \"\"\"\n",
    "    Retrieves the traffic allocation for a specific online endpoint using the provided ML client. \n",
    "\n",
    "    This function fetches the traffic allocation for a given online endpoint using the specified ML client. It returns the traffic allocation as a dictionary where each key-value pair represents the model version and its corresponding percentage of traffic. \n",
    "\n",
    "    Args:\n",
    "    endpoint_name (str): The name of the online endpoint for which to retrieve the traffic allocation.\n",
    "    ml_client (object): An instance of the ML client used to interact with the ML deployment environment. \n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the traffic allocation for the online endpoint, with model versions as keys and traffic percentages as values. \n",
    "    \"\"\"\n",
    "    endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
    "    return endpoint.traffic\n",
    "\n",
    "def get_staged_deployment(endpoint_name, ml_client):\n",
    "    \"\"\"  \n",
    "    Get the staged deployment of a given endpoint.  \n",
    "\n",
    "    This function retrieves the staged deployment with the highest traffic percentage  \n",
    "    for the specified endpoint. If no deployments are found, it returns None.\n",
    "\n",
    "    Parameters:  \n",
    "    endpoint_name (str): The name of the endpoint to retrieve the staged deployment from.  \n",
    "    ml_client (object): The Machine Learning client instance to interact with the API.  \n",
    "\n",
    "    Returns:  \n",
    "    str or None: The name of the staged deployment with the highest traffic percentage,  \n",
    "                 or None if no deployments are found.  \n",
    "    \"\"\"  \n",
    "    endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
    "    mirror_traffic = endpoint.mirror_traffic\n",
    "    if len(mirror_traffic.keys())==0:\n",
    "        return None\n",
    "    else:\n",
    "        staged_deployment = max(mirror_traffic, key=lambda k: mirror_traffic[k])\n",
    "        return staged_deployment\n",
    "\n",
    "def update_traffic(deployment_name, endpoint_name, traffic_percent):\n",
    "    \"\"\"\n",
    "    Updates the traffic allocation for a specific deployment within an online endpoint. \n",
    "\n",
    "    This function updates the traffic percentage for a given deployment within an online endpoint using the provided deployment_name, endpoint_name, and traffic_percent. The traffic allocation is updated using the deployment_client, and the function returns None. \n",
    "\n",
    "    Args:\n",
    "    deployment_name (str): The name of the deployment for which to update the traffic allocation.\n",
    "    endpoint_name (str): The name of the online endpoint containing the deployment.\n",
    "    traffic_percent (int): The new traffic percentage to allocate to the specified deployment. \n",
    "\n",
    "    Returns:\n",
    "    None \n",
    "    \"\"\"\n",
    "    deployment_client = get_deploy_client(mlflow.get_tracking_uri()) \n",
    "    traffic_config = {\"traffic\": {deployment_name: traffic_percent}}\n",
    "    traffic_config_path = \"traffic_config.json\"\n",
    "    \n",
    "    with open(traffic_config_path, \"w\") as outfile:\n",
    "        outfile.write(json.dumps(traffic_config))\n",
    "        \n",
    "    print(f\"Updating traffic to {endpoint_name}\")\n",
    "    print(json.dumps(traffic_config))\n",
    "    deployment_client.update_endpoint(\n",
    "        endpoint=endpoint_name,\n",
    "        config={\"endpoint-config-file\": traffic_config_path},\n",
    "    )\n",
    "    return\n",
    "\n",
    "def update_mirror_traffic(deployment_name, endpoint_name, ml_client, traffic_percent):\n",
    "    \"\"\"\n",
    "    Update the mirror traffic percentage of a deployment in an online endpoint. \n",
    "\n",
    "    This function retrieves the online endpoint using the given endpoint_name, and updates the mirror traffic percentage\n",
    "    of the specified deployment_name within that endpoint. The new traffic percentage is set using the provided\n",
    "    traffic_percent parameter. \n",
    "\n",
    "    Args:\n",
    "    deployment_name (str): The name of the deployment for which the mirror traffic percentage needs to be updated.\n",
    "    endpoint_name (str): The name of the online endpoint containing the specified deployment.\n",
    "    ml_client (MLClient): An instance of MLClient used to manage and interact with the machine learning service.\n",
    "    traffic_percent (float): The new mirror traffic percentage to be assigned to the deployment. \n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"Updating mirror traffic at {endpoint_name}\")\n",
    "    print(json.dumps({deployment_name: traffic_percent}))\n",
    "    endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
    "    endpoint.mirror_traffic = {deployment_name: traffic_percent}\n",
    "    result = ml_client.begin_create_or_update(endpoint).result()\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_current_deployment_name(deployments, model_name):\n",
    "    \"\"\"\n",
    "    Get the current active deployment name for a given model from a dictionary of deployments. \n",
    "\n",
    "    This function takes a dictionary of deployments and the model_name as input, and returns the name of the current\n",
    "    active deployment for the specified model. If there are no deployments, the function returns None. If there are\n",
    "    deployments, the function returns the name of the deployment with the highest traffic percentage. \n",
    "\n",
    "    Args:\n",
    "    deployments (dict): A dictionary containing deployment names as keys and their corresponding traffic percentages as values.\n",
    "    model_name (str): The name of the model for which the current active deployment name is required. \n",
    "\n",
    "    Returns:\n",
    "    str: The name of the current active deployment for the specified model, or None if there are no deployments.\n",
    "    \"\"\"\n",
    "    if len(deployments.keys())==0:\n",
    "        return None\n",
    "    else:\n",
    "        active_deployment = max(deployments, key=lambda k: deployments[k])\n",
    "    return active_deployment\n",
    " \n",
    "def get_new_deployment_name(deployments, model_name):\n",
    "    \"\"\"\n",
    "    Get a new deployment name for a given model from a dictionary of deployments. \n",
    "\n",
    "    This function takes a dictionary of deployments and the model_name as input, and generates a new deployment name\n",
    "    for the specified model. If there are no deployments, the function creates a deployment name with a 'BLUE' prefix.\n",
    "    If there are existing deployments, it checks the prefix of the current active deployment and creates a new deployment\n",
    "    name with the opposite color prefix (either 'BLUE' or 'GREEN'). \n",
    "\n",
    "    Args:\n",
    "    deployments (dict): A dictionary containing deployment names as keys and their corresponding traffic percentages as values.\n",
    "    model_name (str): The name of the model for which the new deployment name is required. \n",
    "\n",
    "    Returns:\n",
    "    str: A new deployment name for the specified model with either a 'BLUE' or 'GREEN' prefix, depending on the current active deployment.\n",
    "    \"\"\"\n",
    "    if len(deployments.keys())==0:\n",
    "        deployment_name = f'BLUE-{model_name}'\n",
    "    else:\n",
    "        active_deployment = max(deployments, key=lambda k: deployments[k])\n",
    "        if 'blue' in active_deployment.lower():\n",
    "            deployment_name = f'GREEN-{model_name}'\n",
    "        else:\n",
    "            deployment_name = f'BLUE-{model_name}'\n",
    "    return deployment_name.lower()\n",
    "\n",
    "def remove_deployment(deployment_name, endpoint_name, ml_client):\n",
    "    \"\"\"\n",
    "    Remove the deployment from an online endpoint. \n",
    "\n",
    "    This function removes the specified deployment_name from the online endpoint using the provided ml_client. The\n",
    "    function begins the delete operation but does not wait for it to complete. \n",
    "\n",
    "    Args:\n",
    "    deployment_name (str): The name of the deployment to be removed.\n",
    "    endpoint_name (str): The name of the online endpoint where the deployment is located.\n",
    "    ml_client (MLClient): An instance of MLClient used to manage and interact with the machine learning service. \n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"Removing deployment {deployment_name} from endpoint {endpoint_name}\")\n",
    "    ml_client.online_deployments.begin_delete(deployment_name, endpoint_name)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e7f542-db28-43c6-9f23-c708e0fd0de6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Promote current staged deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10130207-8e86-4632-b0e0-3d9e7e2b70a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Establish connection to target Azure ML workspace\n",
    "ml_client = get_aml_client(subscription_id, resource_group, workspace)\n",
    "\n",
    "# Get the mlflow tracking URI associated with the AML workspace\n",
    "mlflow_tracking_uri = get_aml_mlflow_tracking_uri(ml_client)\n",
    "\n",
    "# Update MLflow tracking URI\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "# Get current deployments\n",
    "deployments = get_deployments(endpoint_name, ml_client)\n",
    "\n",
    "# Get name of staged deployment (has mirror traffic)\n",
    "staged_deployment_name = get_staged_deployment(endpoint_name, ml_client)\n",
    "\n",
    "# Get name of active deployment (production)\n",
    "active_deployment_name = get_current_deployment_name(deployments, model_name)\n",
    "\n",
    "# Update the endpoint\n",
    "if staged_deployment_name != None:\n",
    "    \n",
    "    # Remove all mirror traffic\n",
    "    update_mirror_traffic(staged_deployment_name, endpoint_name, ml_client, 0)\n",
    "    \n",
    "    # Route all traffic to staged endpoint\n",
    "    update_traffic(staged_deployment_name, endpoint_name, 100)\n",
    "    \n",
    "    # Remove previous deployment\n",
    "    remove_deployment(active_deployment_name, endpoint_name, ml_client)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AML_Update_Deployment",
   "notebookOrigID": 3072769667035568,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
