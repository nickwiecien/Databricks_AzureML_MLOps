trigger:
  branches:
    include:
    - main
  paths:
    include:
    - /

variables:
- group: aml_dbx_variables
  
pool:  
  vmImage: 'ubuntu-latest'  
  
steps:  
- task: UsePythonVersion@0  
  inputs:  
    versionSpec: '3.8'  
    addToPath: true  
  
- task: AzureCLI@2  
  env:
      DATABRICKS_HOST: $(DBX_WORKSPACE)
      DATABRICKS_TOKEN: $(DBX_PAT)
  inputs:  
    azureSubscription: '$(SUBSCRIPTION_SVC_CONNECTION)'  
    scriptType: 'bash'  
    scriptLocation: 'inlineScript'  
    inlineScript: |  

      pip install databricks-cli

      # Authenticate with Azure using service principal  
      az login --service-principal -u $AZURE_SP_APP_ID -p $AZURE_SP_PASSWORD --tenant $AZURE_SP_TENANT  
  
      # Upload notebook to target workspace  
      databricks workspace import $NOTEBOOK_PATH $NOTEBOOK_PATH_DBX -l PYTHON -f JUPYTER -o

      JOB_ID=$(databricks jobs create --json '{
        "name": "TrainingRun",
        "existing_cluster_id": "$(DBX_CLUSTER)",
        "timeout_seconds": 3600,
        "max_retries": 1,
        "notebook_task": {
          "notebook_path": "$(NOTEBOOK_PATH_DBX)",
          "base_parameters":{}
        }
      }' | jq '.job_id')

      echo $JOB_ID

      RUN_ID=$(databricks jobs run-now --job-id $JOB_ID | jq '.run_id')

  
      # Run notebook  
      # databricks workspace execute-command -w $WORKSPACE_ID -c "$NOTEBOOK_PATH_DBX" -o "$NOTEBOOK_PATH_DBX-output.html" -j "$NOTEBOOK_PATH_DBX-job.json" --params "$PARAMETERS"  