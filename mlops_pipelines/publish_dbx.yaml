trigger:
  branches:
    include:
    - main
  paths:
    include:
    - /

variables:
- group: aml_dbx_variables
  
pool:  
  vmImage: 'ubuntu-latest'  
  
steps:  
- task: UsePythonVersion@0  
  inputs:  
    versionSpec: '3.8'  
    addToPath: true  
  
- task: AzureCLI@2  
  env:
      DATABRICKS_HOST: $(DBX_WORKSPACE)
      DATABRICKS_TOKEN: $(DBX_PAT)
  inputs:  
    azureSubscription: '$(SUBSCRIPTION_SVC_CONNECTION)'  
    scriptType: 'bash'  
    scriptLocation: 'inlineScript'  
    inlineScript: |  

      pip install databricks-cli

      # az config set extension.use_dynamic_install=yes_without_prompt

      # Authenticate with Azure using service principal  
      az login --service-principal -u $AZURE_SP_APP_ID -p $AZURE_SP_PASSWORD --tenant $AZURE_SP_TENANT  
  
      # Get workspace ID  
      # WORKSPACE_ID=$(az databricks workspace show --resource-group $RESOURCE_GROUP --name $WORKSPACE_NAME --query id -o tsv)  
  
      # Upload notebook to target workspace  
      databricks workspace import $NOTEBOOK_PATH $NOTEBOOK_PATH_DBX -l PYTHON -f JUPYTER 
  
      # Run notebook  
      # databricks workspace execute-command -w $WORKSPACE_ID -c "$NOTEBOOK_PATH_DBX" -o "$NOTEBOOK_PATH_DBX-output.html" -j "$NOTEBOOK_PATH_DBX-job.json" --params "$PARAMETERS"  